############################################################
### Counterfactual Resilience & Lexicographic Selection: ###
###                                                      ###
###  + Testing counterfactuals' proximity to violations  ###
###    of monotonicity constraints                       ###
###  + Adjusting the fitness function to consider        ###
###    counterfactual resilience as an extension to      ###
###    validity                                          ###
###  + Comparing the effectiveness of lexicographic      ###
###    selection to the Pareto-based approach            ###
###                                                      ###
############################################################

#--- Setup ----
# Set the working directory.
wdir = "C:\\Users\\Owner\\OneDrive\\Documents\\Academic\\UKC\\CS\\3\\All\\COMP6200 Research Project\\MONOMOC\\monomoc\\testing"
setwd(wdir)

# Load necessary library packages.
library("purrr")

# Load helper functions for testing.
source("../helpers/resilience_test_funcs.R")
source("../helpers/lexicographic_test_funcs.R")

# Load counterfactuals package.
devtools::load_all("../counterfactuals", export_all = FALSE)

# Number of metadata columns in MOC results.
NCOLS_CF_METADATA = 5

# Generated by irace in folder appendix_irace.
best.params = readRDS("../saved_objects/best_configs.rds")  

# Data structs.
# Adult Income Census.
adult_data = dataStruct(df = read.csv("datasets/processed/adult_data_less.csv", 
                                      stringsAsFactors = TRUE),
                        id = "adult",
                        target = "class",
                        target_values = c("GT50K", "LTE50K"),
                        nonactionable = c("age", 
                                          "education", 
                                          "marital_status", 
                                          "relationship", 
                                          "race", 
                                          "sex", 
                                          "native_country"))

# COMPAS Recidivism Racial Bias.
compas_data = dataStruct(df = read.csv("datasets/processed/compas_data_less.csv", 
                                       stringsAsFactors = TRUE),
                         id = "compas",
                         target = "two_year_recid",
                         target_values = c("no", "yes"),
                         nonactionable = c("age", "age_cat", "sex", "race"))

# Diabetes.
diabetes_data = dataStruct(df = read.csv("datasets/processed/diabetes_data.csv", 
                                         stringsAsFactors = TRUE),
                           id = "diabetes",
                           target = "outcome",
                           target_values = c("neg", "pos"),
                           nonactionable = c("age", "pregnancies"))

# HELOC (FICO 2018).
fico_data = dataStruct(df = read.csv("datasets/processed/fico_data_less.csv", 
                                     stringsAsFactors = TRUE),
                       id = "fico",
                       target = "riskperformance",
                       target_values = c("Good", "Bad"),
                       nonactionable = c("externalriskestimate"))

# German credit risk.
german_data = dataStruct(df = read.csv("datasets/processed/german_credit_data.csv", 
                                       stringsAsFactors = TRUE),
                         id = "german",
                         target = "risk",
                         target_values = c("good", "bad"),
                         nonactionable = c("age", "sex"))

# ML Algorithm Types.
# Neural network (NN).
nn_alg = mlAlgStruct(id = NN_ALG_ID,
                     target_range = c(0.5, 1.0))

# Random Forest (RF)
rf_alg = mlAlgStruct(id = RF_ALG_ID,
                     target_range = c(0.5, 1.0))

# Support Vector Machine (SVM).
svm_alg = mlAlgStruct(id = SVM_ALG_ID,
                      target_range = c(0.5, 1.0))

# Full list of datasets.
DATASETS = list(german_data)

# Full list of ML algorithm types.
ML_ALGS = list(nn_alg)

# Main test function for monotonicity constraint violation proximity (resilience)
# and comparisons to the lexicographic selection function. 
run <- function(data,
                data_id,
                target,
                target_values,
                naf,
                ml_alg_id,
                ml_alg_target_range,
                n_points_of_interest = 1,
                data_feat_types = sapply(type.convert(data, as.is = TRUE), class),
                TD_PADDING_MULTIPLIER = 5,
                ext.resilience = FALSE,
                lexicographic.selection = FALSE) 
  {
  
  # Check parameters.
  if ((n_points_of_interest != round(n_points_of_interest))|
      (TD_PADDING_MULTIPLIER != round(TD_PADDING_MULTIPLIER))) {
    stop("Error: n_points_of_interest & TD_PADDING_MULTIPLIER must be integers")
  }
  if (n_points_of_interest < 1) {
    warning("Warning: Setting n_points_of_interest to at least 1")
    n_points_of_interest = 1
  }
  if (TD_PADDING_MULTIPLIER < 1) {
    warning("Warning: Setting TD_PADDING_MULTIPLIER to at least 1")
    TD_PADDING_MULTIPLIER = 1
  }
  if ((n_points_of_interest * TD_PADDING_MULTIPLIER) > (nrow(data) %/% 2)) {
    stop(sprintf("Error: n_points_of_interest can be no greater than %d%% of nrow(data)", 
                  50 / TD_PADDING_MULTIPLIER))
  }
  
  # Count of counterfactuals (valid or not), for each (dataset, model) pair.
  cf_count = 0
  
  # Count of valid counterfactuals.
  cf_valid_count = 0
  
  # Count of counterfactuals that undergo resilience tests.
  cf_test_count = 0
  
  # Count of invalid CFs, for each (dataset, model) pair.
  cf_inv_count = 0  
  
  # Resilience data frame.
  cf_nms = colnames(data[,-which(names(data) == target)])
  resilience_df = setNames(data.frame(matrix(ncol = length(cf_nms), nrow = 0)), cf_nms)
  
  # Count of usable data samples tested.
  points_of_interest_tested = 0
  
  # Count of unusable data samples.
  points_of_interest_rejected = 0
  poi_pred_positive_class = 0
  poi_returned_no_cfs = 0
  poi_returned_no_valid_cfs = 0
  poi_returned_fully_categ_cfs = 0
  
  # Minimum & maximum values for each numeric feature in the test data.
  min_feat_values = apply(data, 2, min)
  min_feat_values = map_if(min_feat_values, isNumericString, as.numeric)
  max_feat_values = apply(data, 2, max)
  max_feat_values = map_if(max_feat_values, isNumericString, as.numeric)
  
  print("\nMIN FEAT VALUES:")
  print(min_feat_values)
  print("\nMAX FEAT VALUES:")
  print(max_feat_values)
  
  # Select training & test data.
  set.seed(as.numeric(Sys.time()))
  test_data_idx = sample(1:nrow(data), n_points_of_interest * TD_PADDING_MULTIPLIER)
  train_data = data[-test_data_idx,]
  test_data = data[test_data_idx,]
  
  # Remaining test data idx.
  test_data_idx = 1:nrow(data)[-test_data_idx]
  
  # Create predictor.
  pred = getPredictor(ml_alg_id = ml_alg_id, 
                      data = train_data,
                      data_id = data_id,
                      target = target,
                      target_values = target_values)
  
  # Operate on points of interest.
  while (points_of_interest_tested < n_points_of_interest) {

    # Randomly sample a usable data point.
    is_usable = FALSE
    while (!is_usable) {

      # No more usable points of interest in current test data segment.
      if (nrow(test_data) == 0) {
        
        # Terminate early, or...
        if (length(test_data_idx) == 0) {
          warning("Warning: Terminated before full points of interest set was tested")
          return(generateResilienceTestResults(resilience_df,
                                               cf_count,
                                               cf_valid_count,
                                               cf_test_count,
                                               cf_inv_count,
                                               points_of_interest_tested,
                                               points_of_interest_rejected,
                                               poi_pred_positive_class,
                                               poi_returned_no_cfs,
                                               poi_returned_no_valid_cfs,
                                               poi_returned_fully_categ_cfs))
        }
        
        print("Retraining...")
        
        # Set new test data segment size.
        td_segment_size = n_points_of_interest * TD_PADDING_MULTIPLIER
        if (length(test_data_idx) < td_segment_size) {
          td_segment_size = length(test_data_idx)
        }
        
        # Re-segment data into training & test.
        set.seed(as.numeric(Sys.time()))
        new_test_data_idx = sample(length(test_data_idx), td_segment_size)
        test_data = data[test_data_idx[new_test_data_idx],]
        train_data = data[-test_data_idx[new_test_data_idx],]
        
        # Remove new test data idx from remaining test data idx.
        test_data_idx = test_data_idx[-new_test_data_idx]
        
        # Train new predictor.
        pred = getPredictor(ml_alg_id,
                            train_data,
                            data_id,
                            target,
                            target_values)
      }
      
      # Select point of interest.
      set.seed(as.numeric(Sys.time()))
      sample_i = sample(1:nrow(test_data), 1)
      x.interest = test_data[sample_i,]
      
      # Remove point of interest from test data.
      test_data = test_data[-sample_i,]
      
      # Verify that the point of interest is predicted as negative class.
      print(x.interest)
      result = pred$predict(x.interest)
      print(result)
      
      if (isNegativeClass(result, ml_alg_target_range)) {
        is_usable = TRUE
      }
      else {
        # Predicted positive class: Data point unusable.
        points_of_interest_rejected = points_of_interest_rejected + 1
        poi_pred_positive_class = poi_pred_positive_class + 1
      }
    }
    
    print("\nPoint of Interest: ")
    print(x.interest)
    
    # Compute counterfactuals for data point of interest.
    set.seed(1000)
    system.time({data.cf = Counterfactuals$new(predictor = pred, 
                                               x.interest = x.interest,
                                               target = ml_alg_target_range, 
                                               epsilon = 0, 
                                               generations = list(mosmafs::mosmafsTermStagnationHV(10),
                                                                  mosmafs::mosmafsTermGenerations(200)), 
                                               mu = best.params$mu, 
                                               p.mut = best.params$p.mut, p.rec = best.params$p.rec, 
                                               p.mut.gen = best.params$p.mut.gen, 
                                               p.mut.use.orig = best.params$p.mut.use.orig, 
                                               p.rec.gen = best.params$p.rec.gen, initialization = "icecurve",
                                               p.rec.use.orig = best.params$p.rec.use.orig,
                                               fixed.features = naf,
                                               lexicographic.selection = lexicographic.selection,
                                               ext.resilience = ext.resilience)})
    
    browser()
    
    # No counterfactuals returned.
    if (is.null(data.cf)) {
      points_of_interest_rejected = points_of_interest_rejected + 1
      poi_returned_no_cfs = poi_returned_no_cfs + 1
      print("###############################")
      print("# No counterfactuals returned #")
      print("###############################")
    }
    else {
      
      browser()
      
      # Increment total count of counterfactuals.
      num_cf = nrow(data.cf$results$counterfactuals)
      cf_count = cf_count + num_cf
      
      print("Counterfactuals: ")
      print(data.cf$results$counterfactuals)
      
      # Remove invalid counterfactuals.
      valid_cf_idx = data.cf$results$counterfactuals$dist.target == 0
      data.cf$results$counterfactuals = data.cf$results$counterfactuals[which(valid_cf_idx), ]
      data.cf$results$counterfactuals.diff = data.cf$results$counterfactuals.diff[which(valid_cf_idx), ]
      
      # Increment count of valid counterfactuals.
      cf_valid_count = cf_valid_count + nrow(data.cf$results$counterfactuals)
      
      # Increment count of invalid counterfactuals.
      cf_inv_count = cf_inv_count + (num_cf - nrow(data.cf$results$counterfactuals))
      
      # Get relative frequency of feature changes.
      data.cf$get_frequency()
      
      # Remove additional columns.
      valid_cfs = data.cf$results$counterfactuals[,1:(ncol(data.cf$results$counterfactuals[,]) - NCOLS_CF_METADATA)]
      
      # Verification that at least one counterfactual has been tested.
      valid_cf_tested = FALSE
      
      # Verification that a fully categorical counterfactual has been returned.
      fully_categ_cf = FALSE
      
      # For each valid counterfactual...
      j = 0
      while (j < nrow(valid_cfs)) {
        j = j + 1
        cf = valid_cfs[j,]
        
        print("\nOriginal CF:")
        print(cf)
        
        # Test original counterfactual.
        result = pred$predict(cf)
        print(result)
        
        # Calculate resilience for the current counterfactual.
        cf_resilience_df = getResilience(x.interest[,-which(names(x.interest) == target)],
                                         cf,
                                         pred,
                                         ml_alg_target_range,
                                         max_feat_values,
                                         min_feat_values,
                                         data_feat_types)
        
        # CF has mutated numerical features: Resilience testable. 
        if (!is.null(cf_resilience_df)) {
          valid_cf_tested = TRUE
          cf_test_count = cf_test_count + 1
          resilience_df[cf_test_count,] = cf_resilience_df
        }
        else {
          # Rejected: Only mutated categorical features.
          fully_categ_cf = TRUE
        }
      }
      
      # If at least one valid counterfactual underwent a resilience test...
      if (valid_cf_tested) {
        points_of_interest_tested = points_of_interest_tested + 1
      }
      else if (fully_categ_cf) {
        points_of_interest_rejected = points_of_interest_rejected + 1
        poi_returned_fully_categ_cfs = poi_returned_fully_categ_cfs + 1
      }
      else if (j == 0) {
        points_of_interest_rejected = points_of_interest_rejected + 1
        poi_returned_no_valid_cfs = poi_returned_no_valid_cfs + 1
      }
    }
  }

  return(generateResilienceTestResults(resilience_df,
                                       cf_count,
                                       cf_valid_count,
                                       cf_test_count,
                                       cf_inv_count,
                                       points_of_interest_tested,
                                       points_of_interest_rejected,
                                       poi_pred_positive_class,
                                       poi_returned_no_cfs,
                                       poi_returned_no_valid_cfs,
                                       poi_returned_fully_categ_cfs))
}

# Test datasets x ML alg types.
for (ds in DATASETS) {
  # Prepare data.
  names(ds@df) = tolower(names(ds@df))
  ds@df = na.omit(ds@df)
  ds@df[,ds@target] = as.factor(ds@df[,ds@target])
  
  for (ml_alg in ML_ALGS) {
    sink(sprintf("logs/resilience_tests_%s_%s.txt", ds@id, ml_alg@id))
    print(nrow(ds@df))
    start_time = Sys.time()
    results = run(ds@df,
                  ds@id,
                  ds@target,
                  ds@target_values,
                  ds@nonactionable,
                  ml_alg@id,
                  ml_alg@target_range,
                  n_points_of_interest = 1,
                  TD_PADDING_MULTIPLIER = 5)
    end_time = Sys.time()
    print("\nRESULTS: ")
    print(results)
    writeLines(sprintf("Execution Time: %f", (end_time - start_time)))
    sink()
  }
}