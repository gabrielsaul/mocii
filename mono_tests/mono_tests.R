############################################
### Monotonicity Tests
############################################

# Datasets (D):
# - German Credit Risk
# - Diabetes
# - Adult Income Census
# - HELOC (FICO 2018)
# - COMPAS Recidivism Racial Bias

# AI Model Algorithms (F):
# - Support Vector Machine (SVM)
# - Neural Network (NN)
# - Random Forest (RF)


# D = c(“datasets/german_credit.csv”, 
#       “datasets/diabetes.csv”, 
#       “datasets/adult.csv”, 
#       “datasets/fico.csv”, 
#       “datasets/compas-scores-two-years.csv")

# F = c("svm", "nn", "rf")

# Misc. constants.
POS_CLV_INDEX = 1
NEG_CLV_INDEX = 2

# Datasets w/ class labels & non-actionable features.
# German credit risk
german_fp <- "datasets/german_credit_data.csv"
german_id <- "german"
german_class_label <- "risk"
german_class_values <- c("good", "bad")
german_naf <- c("age", "sex")

# Diabetes
diabetes_fp <- "datasets/diabetes.csv"
diabetes_id <- "diabetes"
diabetes_class_label <- "outcome"
diabetes_naf <- c("age", "pregnancies")

# Adult Income Census
adult_fp <- "datasets/adult.csv"
adult_id <- "adult"
adult_class_label <- "class"
adult_naf <- c("age", "education", "marital status", "relationship", "race", "sex", "native country")

# HELOC (FICO 2018)
fico_fp <- "datasets/fico.csv"
fico_id <- "fico"
fico_class_label <- "RiskPerformance"
fico_naf <- c("ExternalRiskEstimate")

# COMPAS Recidivism Racial Bias
compas_fp <- "datasets/compas.csv"
compas_id <- "compas"
compas_class_label <- "two_year_recid"
compas_naf <- c("age", "sex", "race")

# ML Algorithm Types.
# Support Vector Machine (SVM).
SVM_ALG <- "svm"

# Neural network (NN).
NN_ALG <- "nn"

# Random Forest (RF)
RF_ALG <- "rf"

#--- Setup ----
# Set the working directory.
wdir = "C:\\Users\\Owner\\OneDrive\\Documents\\Academic\\UKC\\CS\\3\\All\\COMP6200 Research Project\\MONOMOC\\monomoc\\mono_tests"
setwd(wdir)

# To run MOC
# load `iml` and `counterfactuals` like "normal" packages.
devtools::load_all("../iml", export_all = FALSE)
devtools::load_all("../counterfactuals", export_all = FALSE)

# Load necessary library packages.
library("mlr")
library("mlrCPO")
library("ggplot2")
library("purrr")

# Generated by irace in folder appendix_irace.
best.params = readRDS("../saved_objects/best_configs.rds")  
PARALLEL = TRUE

# Insert the given new row into the given data frame after the given row index.
insertRow <- function(df, new_row, r) { 
  df_new <- rbind(df[1:r, ], new_row, df[- (1:r), ])         
  rownames(df_new) <- 1:nrow(df_new)     
  return(df_new) 
} 

# Return TRUE if the given string is numeric, else FALSE.
isNumericString <- function(s) {
  suppressWarnings(return(!is.na(as.numeric(s))))
}

# Return TRUE if the given result equates to a negative class prediction,
# else return FALSE.
isNegativeClass <- function(res, ml_alg_id) {
  if (ml_alg_id == SVM_ALG) {
    return(res < 0.5)
  }
  else {
    return(FALSE)
  }
}

monoTest <- function(test_data,
                     test_data_id,
                     test_data_class_label,
                     test_data_class_values,
                     test_data_naf,
                     ml_alg_id) {
  # Count of counterfactuals (valid or not), for each (dataset, model) pair.
  cf_count = 0

  # Count of invalid CFs, for each (dataset, model) pair.
  cf_inv_count = 0  
  
  # Count of mutations before an MV is encountered for each (dataset, model) pair.
  avg_monot_mut_step_count = 0 
  
  # Total number of mutated features for each (model, dataset) pair accumulated 
  # across all valid CFs and all points of interest
  mut_feat_count = 0
  
  # Maximum number of mutations for MV searches.
  monot_mut_steps = 10
  
  # Number of sampled data points of interest.
  num_points_interest = 1
  
  # Base sample set of all data points with negative class values.
  base_sample_set = which(test_data[,test_data_class_label] == test_data_class_values[NEG_CLV_INDEX])
  
  # Minimum & maximum values for each feature in the test data.
  min_feat_values = apply(test_data, 2, min)
  min_feat_values = map_if(min_feat_values, isNumericString, as.numeric)
  max_feat_values = apply(test_data, 2, max)
  max_feat_values = map_if(max_feat_values, isNumericString, as.numeric)
  
  # Create output file
  sink(sprintf("mono_tests_%s_%s.txt", test_data_id, ml_alg_id))

  for (i in 1:num_points_interest) {

    # Randomly sample a usable data point. 
    is_usable = FALSE
    while (!is_usable) {
      
      # Select point of interest.
      sample_i = sample(1:length(base_sample_set), 1)
      x.interest = test_data[base_sample_set[sample_i],]
      
      # Remove point of interest from training data & base sample set.
      test_data = test_data[-base_sample_set[sample_i],]
      base_sample_set = base_sample_set[-sample_i]
      
      # Train the model.
      if (ml_alg_id == SVM_ALG) {
        test_data.task = makeClassifTask(id = test_data_id,
                                      data = test_data, target = test_data_class_label)
        lrn = makeLearner("classif.svm", predict.type = "prob")
        test_data.lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
        param.set = pSS(
          cost: numeric[0.01, 1]
        )
        
        TUNEITERS = 100L
        RESAMPLING = cv5
        
        if (PARALLEL) {
          parallelMap::parallelStartSocket(parallel::detectCores(), level = "mlr.tuneParams")
        }
        
        ctrl = makeTuneControlRandom(maxit = TUNEITERS * length(param.set$pars))
        lrn.tuning = makeTuneWrapper(lrn, RESAMPLING, list(mlr::acc), param.set, ctrl, show.info = FALSE)
        res = tuneParams(lrn, test_data.task, RESAMPLING, par.set = param.set, control = ctrl,
                         show.info = FALSE)
        performance = resample(lrn.tuning, test_data.task, RESAMPLING, list(mlr::acc))$aggr
        
        if (PARALLEL) {
          parallelMap::parallelStop()
        }
        test_data.lrn = setHyperPars2(test_data.lrn, res$x) 
        test_data.model = mlr::train(test_data.lrn, test_data.task)
      }
  
      pred = Predictor$new(model = test_data.model, data = test_data, class = test_data_class_values[POS_CLV_INDEX],
                           conditional = FALSE)
      ctr = partykit::ctree_control(maxdepth = 5L)
      
      set.seed(1234)
      pred$conditionals = fit_conditionals(pred$data$get.x(), ctrl = ctr)
      
      # Verify that the point of interest is predicted as negative class.
      res <- pred$predict(x.interest)
      if (isNegativeClass(res, ml_alg_id)) {
        is_usable = TRUE
      }
      else if (length(base_sample_set) > 0) {
        # Add data point back to test data.
        test_data = insertRow(test_data, x.interest, sample_i - 1)
      }
      else {
        # No more usable samples. Terminate.
        return()
      }
    }
    
    # Compute counterfactuals for data point of interest.
    set.seed(1000)
    system.time({test_data.cf = Counterfactuals$new(predictor = pred, 
                                                 x.interest = x.interest, 
                                                 target = c(0.5, 1), epsilon = 0, generations = list(mosmafs::mosmafsTermStagnationHV(10),
                                                                                                     mosmafs::mosmafsTermGenerations(200)), 
                                                 mu = best.params$mu, 
                                                 p.mut = best.params$p.mut, p.rec = best.params$p.rec, 
                                                 p.mut.gen = best.params$p.mut.gen, 
                                                 p.mut.use.orig = best.params$p.mut.use.orig, 
                                                 p.rec.gen = best.params$p.rec.gen, initialization = "icecurve",
                                                 p.rec.use.orig = best.params$p.rec.use.orig,
                                                 fixed.features = test_data_naf)})
    
    # Number of counterfactuals.
    nrow(test_data.cf$results$counterfactuals)
    id = test_data.cf$results$counterfactuals$dist.target == 0
    sum(id)
    
    # Focus counterfactuals that met target.
    test_data.cf$results$counterfactuals = test_data.cf$results$counterfactuals[which(id), ]
    test_data.cf$results$counterfactuals.diff = test_data.cf$results$counterfactuals.diff[which(id), ]
    
    # Get relative frequency of feature changes.
    test_data.cf$get_frequency()

    # For each counterfactual...
    for (j in 1:nrow(test_data.cf$results$counterfactuals)) {
      
      # Increment number of counterfactuals.
      cf_count <- cf_count + 1
      
      # Remove additional columns.
      cf <- test_data.cf$results$counterfactuals[j,][,1:(ncol(test_data.cf$results$counterfactuals[j,]) - 5)]
      
      print("Original CF:")
      print(cf)
      
      # Test original counterfactual.
      res <- pred$predict(cf)
      
      # Invalid counterfactual found.
      if (isNegativeClass(res, ml_alg_id)) {
        cf_inv_count <- cf_inv_count + 1
        writeLines(sprintf("!x! INVALID COUNTERFACTUAL #%d", cf_inv_count))
      }
      else {
        
        # For every mutated numerical feature...
        for (k in names(cf)) {
          
          if (is.numeric(cf[[k]]) & (cf[[k]] != x.interest[[k]])) {
            writeLines(sprintf("MUT FEATURE: %s", k))
            
            # Calculate increment.
            # NOTE: How to handle increments below 0.
            #       Produced if the difference between the original feature
            #       and the counterfactual is minimal.
            inc = 0
            if (cf[[k]] < x.interest[[k]]) {
              # Adjust minimum.
              if (cf[[k]] < min_feat_values[[k]]) {
                print("MIN CHANGED")
                min_feat_values[[k]] = cf[[k]]
              }
              inc = (min_feat_values[[k]] - cf[[k]]) %/% monot_mut_steps
            }
            else {
              # Adjust maximum.
              if (cf[[k]] > max_feat_values[[k]]) {
                print("MAX CHANGED")
                max_feat_values[[k]] = cf[[k]]
              }
              inc = (max_feat_values[[k]] - cf[[k]]) %/% monot_mut_steps
            }
            
            print("Increment: ")
            print(inc)
      
            # Mutate feature in steps to search for an MV.
            monot_mut_step_count = 0
            for (s in 1:monot_mut_steps) {
              cf[k] <- cf[k] + inc
              print(cf)
              
              res <- pred$predict(cf)
              if (isNegativeClass(res, ml_alg_id)) {
                writeLines(sprintf("!v! MV FOUND, STEPS: #%d", s))
                break;
              }
              monot_mut_step_count <- monot_mut_step_count + 1
              avg_monot_mut_step_count <- avg_monot_mut_step_count + 1
            }
            print("\n#######################################################\n")
          }
        }
      }
    }
  
    # Add point of interest back to test data.
    test_data = insertRow(test_data, x.interest, i - 1)
  }
  
  sink()
}

# debug(monoTest)

###---- Set & preprocess data ----
test_data <- read.csv(german_fp, row.names = 1, stringsAsFactors = TRUE)
names(test_data)

# Omit rows with NA entries.
test_data = na.omit(test_data)

# Join groups with small frequencies.
levels(test_data$Purpose) = c("others", "car", "others", "others",
                              "furniture", "radio/TV", "others", "others")
levels(test_data$Saving.accounts) = c("little", "moderate", "rich", "rich")

# Column names to lower case.
names(test_data) = tolower(names(test_data))

# Drop levels.
test_data = droplevels.data.frame(test_data)

# Test dataset w/ algorithm type.
monoTest(test_data,
         german_id,
         german_class_label,
         german_class_values,
         german_naf,
         SVM_ALG)