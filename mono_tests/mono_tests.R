############################################
### Monotonicity Tests
############################################

# Indexing constants.
POS_CLV_INDEX = 1
NEG_CLV_INDEX = 2
LOW_TARG_RANGE = 1
UPP_TARG_RANGE = 2

# Struct class for datasets.
dataStruct <- setClass(
  "DataStruct",
  slots = c(fp = "character",
  id = "character",
  target = "character",
  target_values = "character",
  nonactionable = "character")
)

# Struct class for ML algorithm types.
mlAlgStruct <- setClass(
  "MLAlg",
  slots = c(id = "character",
            target_range = "numeric")
)

# Struct class for resilience results.
monoTestResults <- setClass(
  "monoTestResults",
  slots = c(resilience = "numeric",
            counterfactuals = "numeric",
            invalid = "numeric",
            mutated = "numeric",
            features = "numeric",
            instances = "numeric")
)

# Datasets w/ class labels & non-actionable features.
# German credit risk.

# Adult Income Census.
adult_data = dataStruct(fp = "datasets/processed/adult_data_less.csv",
                        id = "adult",
                        target = "class",
                        target_values = c("GT50K", "LTE50K"),
                        nonactionable = c("age", 
                                          "education", 
                                          "marital_status", 
                                          "relationship", 
                                          "race", 
                                          "sex", 
                                          "native_country"))

# COMPAS Recidivism Racial Bias.
compas_data = dataStruct(fp = "datasets/processed/compas_data.csv",
                         id = "compas",
                         target = "two_year_recid",
                         target_values = c("no", "yes"),
                         nonactionable = c("age", "age_cat", "sex", "race"))

# Diabetes.
diabetes_data = dataStruct(fp = "datasets/processed/diabetes_data.csv",
                           id = "diabetes",
                           target = "outcome",
                           target_values = c("neg", "pos"),
                           nonactionable = c("age", "pregnancies"))

# HELOC (FICO 2018).
fico_data = dataStruct(fp = "datasets/processed/fico_data.csv",
                       id = "fico",
                       target = "riskperformance",
                       target_values = c("Good", "Bad"),
                       nonactionable = c("externalriskestimate"))

german_data = dataStruct(fp = "datasets/processed/german_credit_data.csv",
                         id = "german",
                         target = "risk",
                         target_values = c("good", "bad"),
                         nonactionable = c("age", "sex"))

# ML Algorithm Types.
# Neural network (NN).
NN_ALG_ID = "nn"

# Random Forest (RF)
RF_ALG_ID = "rf"
rf_alg = mlAlgStruct(id = RF_ALG_ID,
                     target_range = c(1, 1))

# Support Vector Machine (SVM).
SVM_ALG_ID = "svm"
svm_alg = mlAlgStruct(id = SVM_ALG_ID,
                     target_range = c(0.5, 1.0))

# Full list of datasets.
DATASETS = list(german_data)

# Full list of ML algorithm types.
ML_ALGS = list(svm_alg)

#--- Setup ----
# Set the working directory.
wdir = "C:\\Users\\Owner\\OneDrive\\Documents\\Academic\\UKC\\CS\\3\\All\\COMP6200 Research Project\\MONOMOC\\monomoc\\mono_tests"
setwd(wdir)

# To run MOC
# load `iml` and `counterfactuals` like "normal" packages.
devtools::load_all("../iml", export_all = FALSE)
devtools::load_all("../counterfactuals", export_all = FALSE)

# Load necessary library packages.
library("mlr")
library("neuralnet")
library("randomForest")
library("mlrCPO")
library("ggplot2")
library("purrr")
library("dplyr")

# Generated by irace in folder appendix_irace.
best.params = readRDS("../saved_objects/best_configs.rds")  
PARALLEL = TRUE

# Return a formula representing the dependency of a target variable on
# a set of predictor variables, provided or defaulted to all other features.
getClassifFormula <- function(target_label = "", 
                              target_value = "", 
                              predictors = ".") {
  targ = target_label
  if (target_value != "") {
    if (target_label == "") {
      stop("Error: No target label provided for value comparison in getClassifFormula()")
    }
    
    q = ""
    if (!is.numeric(target_value)) {
      q = "\""
    }
    targv = gsub(" ", "", paste(q, target_value, q))
    targ = paste("(", target_label, " == ", targv, ")", collapse = "")
  }
  
  form = paste(targ, " ~ ", paste(predictors, collapse = " + "))
  return(as.formula(form))
}

# Return a hashmap initialised to the given value for the given set of column 
# names.
getHashMap <- function(nms, val = 0) {
  hm = c()
  for (nm in nms) {
    hm[nm] = val
  }
  return(hm)
}

# Return the names of all columns with different numeric values in the given
# counterfactual when compared to the other given data point.
getMutNumericFeatures <- function(cf, orig) {
  if (ncol(cf) != ncol(orig)) {
    return(FALSE)
  }
  
  nms = c()
  for (nm in names(cf)) {
    if (is.numeric(cf[[nm]]) & cf[[nm]] != orig[[nm]]) {
      nms = append(nms, nm)
    }
  }
  return(nms)
}

# Insert the given new row into the given data frame after the given row index.
insertRow <- function(df, new_row, r) { 
  df_new = rbind(df[1:r, ], new_row, df[- (1:r), ])         
  rownames(df_new) = 1:nrow(df_new)     
  return(df_new) 
} 

# Return TRUE if the given string is numeric, else FALSE.
isNumericString <- function(s) {
  suppressWarnings(return(!is.na(as.numeric(s))))
}


# Return TRUE if the given result equates to a negative class prediction,
# else return FALSE.
isNegativeClass <- function(result, data_target_values, ml_alg_target_range) {
  lower = ml_alg_target_range[LOW_TARG_RANGE]
  upper = ml_alg_target_range[UPP_TARG_RANGE]
  if (names(result)[1] == data_target_values[POS_CLV_INDEX]) {
    return(!(result >= lower & result <= upper))
  }
  else if (names(result)[1] == data_target_values[NEG_CLV_INDEX]) {
    return(result >= lower & result <= upper)
  }
  else {
    stop("Error: Invalid predictor result passed to isNegativeClass")
  }
}

monoTest <- function(data,
                     data_id,
                     data_target,
                     data_target_values,
                     data_naf,
                     ml_alg_id,
                     ml_alg_target_range,
                     num_points_interest = 1,
                     data_feat_types = sapply(type.convert(data, as.is = TRUE), class)
                     ) {

  # Count of counterfactuals (valid or not), for each (dataset, model) pair.
  cf_count = 0

  # Count of invalid CFs, for each (dataset, model) pair.
  cf_inv_count = 0  
  
  # Hashmap of feature names to successful MV step ratios.
  feat_resilience_scores = getHashMap(names(data[,-which(names(data) == data_target)]))
  
  # Hashmap of feature names to count of mutated instances.
  feat_mut_count = getHashMap(names(feat_resilience_scores))
  
  # Base sample set of all data points with negative class values.
  base_sample_set = which(data[,data_target] == data_target_values[NEG_CLV_INDEX])

  # Minimum & maximum values for each numeric feature in the test data.
  min_feat_values = apply(data, 2, min)
  min_feat_values = map_if(min_feat_values, isNumericString, as.numeric)
  max_feat_values = apply(data, 2, max)
  max_feat_values = map_if(max_feat_values, isNumericString, as.numeric)
  
  # Formula for new model calls.
  form = as.formula(paste(data_target, 
                          " ~ ", 
                          paste(names(data)[names(data) != data_target], collapse = " + ")))

  # Operate on points of interest.
  for (i in 1:num_points_interest) {

    # Randomly sample a usable data point. 
    is_usable = FALSE
    while (!is_usable) {
      
      # Select point of interest.
      set.seed(as.numeric(Sys.time()))
      sample_i = sample(1:length(base_sample_set), 1)
      x.interest = data[base_sample_set[sample_i],]
      
      # Remove point of interest from training data & base sample set.
      data = data[-base_sample_set[sample_i],]
      base_sample_set = base_sample_set[-sample_i]
      
      # Choose & train the model and set the predictor.
      pred = NULL
      if (ml_alg_id == RF_ALG_ID) {
        set.seed(as.numeric(Sys.time()))
        rf = randomForest(form, importance = TRUE, data = data)
        pred = Predictor$new(model = rf, 
                             data = data, 
                             class = as.factor(data_target_values[POS_CLV_INDEX]), 
                             conditional = FALSE)
      }
      else if (ml_alg_id == NN_ALG_ID) {
        set.seed(as.numeric(Sys.time()))
        return()
      }
      else if (ml_alg_id == SVM_ALG_ID) {
        data.task = makeClassifTask(id = data_id,
                                         data = data,
                                         target = data_target,
                                         positive = data_target_values[POS_CLV_INDEX]
                                    )
        lrn = makeLearner("classif.svm", predict.type = "prob")
        data.lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
        param.set = pSS(
          cost: numeric[0.01, 1]
        )
        
        TUNEITERS = 100L
        RESAMPLING = cv5
        
        if (PARALLEL) {
          parallelMap::parallelStartSocket(parallel::detectCores(), level = "mlr.tuneParams")
        }
        
        ctrl = makeTuneControlRandom(maxit = TUNEITERS * length(param.set$pars))
        lrn.tuning = makeTuneWrapper(lrn, RESAMPLING, list(mlr::acc), param.set, ctrl, show.info = FALSE)
        res = tuneParams(lrn, data.task, RESAMPLING, par.set = param.set, control = ctrl,
                         show.info = FALSE)
        performance = resample(lrn.tuning, data.task, RESAMPLING, list(mlr::acc))$aggr
        
        if (PARALLEL) {
          parallelMap::parallelStop()
        }
        data.lrn = setHyperPars2(data.lrn, res$x) 
        data.model = mlr::train(data.lrn, data.task)
        
        pred = Predictor$new(model = data.model, 
                             data = data, 
                             class = data_target_values[POS_CLV_INDEX],
                             conditional = FALSE)
  
        ctr = partykit::ctree_control(maxdepth = 5L)
        
        set.seed(1234)
        pred$conditionals = fit_conditionals(pred$data$get.x(), ctrl = ctr)
      }
      else {
        stop("Error: Invalid ML algorithm ID")
      }
      
      # Verify that the point of interest is predicted as negative class.
      result = pred$predict(x.interest)
      print(result)

      if (isNegativeClass(result, data_target_values, ml_alg_target_range)) {
        is_usable = TRUE
      }
      else if (length(base_sample_set) > 0) {
        # Add data point back to test data.
        data = insertRow(data, x.interest, sample_i - 1)
      }
      else {
        # No more usable samples. Terminate.
        warning("NOTE: Terminated before full sample set was tested.")
        return(c(cf_count, cf_inv_count, mut_feat_count, avg_monot_mut_step_count))
      }
    }
    
    # Point of interest selected.
    print("Point of Interest: ")
    print(x.interest)
    
    # Compute counterfactuals for data point of interest.
    set.seed(1000)
    system.time({data.cf = Counterfactuals$new(predictor = pred, 
                                               x.interest = x.interest,
                                               target = ml_alg_target_range, epsilon = 0, generations = list(mosmafs::mosmafsTermStagnationHV(10),
                                                                                                     mosmafs::mosmafsTermGenerations(200)), 
                                               mu = best.params$mu, 
                                               p.mut = best.params$p.mut, p.rec = best.params$p.rec, 
                                               p.mut.gen = best.params$p.mut.gen, 
                                               p.mut.use.orig = best.params$p.mut.use.orig, 
                                               p.rec.gen = best.params$p.rec.gen, initialization = "icecurve",
                                               p.rec.use.orig = best.params$p.rec.use.orig,
                                               fixed.features = data_naf)})
    
    # Increment number of counterfactuals.
    num_cf = nrow(data.cf$results$counterfactuals)
    cf_count = cf_count + num_cf
    id = data.cf$results$counterfactuals$dist.target == 0
    sum(id)
    
    print("Counterfactuals: ")
    print(data.cf$results$counterfactuals)
    
    # Count invalid counterfactuals & remove from set.
    data.cf$results$counterfactuals = data.cf$results$counterfactuals[which(id), ]
    data.cf$results$counterfactuals.diff = data.cf$results$counterfactuals.diff[which(id), ]
    cf_inv_count = cf_inv_count + (num_cf - nrow(data.cf$results$counterfactuals))
    
    # Get relative frequency of feature changes.
    data.cf$get_frequency()

    # For each counterfactual...
    for (j in 1:nrow(data.cf$results$counterfactuals)) {
      
      # Remove additional columns.
      cf = data.cf$results$counterfactuals[j,][,1:(ncol(data.cf$results$counterfactuals[j,]) - 5)]
      
      print("Original CF:")
      print(cf)
      
      # Test original counterfactual.
      result = pred$predict(cf)
      print(result)

      # Calculate mutated numeric features.
      tmp_x = x.interest[,-which(names(x.interest) == data_target)]
      changed = getMutNumericFeatures(cf, tmp_x)
      writeLines(sprintf("Number of Mutated Features: %f", length(changed)))

      # For each mutated numeric feature...
      for (k in changed) {
        # If already at min/max, skip.
        if (cf[[k]] > min_feat_values[[k]] & cf[[k]] < max_feat_values[[k]]) {
          writeLines(sprintf("Mutated Feature: %s", k))
          
          # Increment count of instances of mutated feature.
          feat_mut_count[k] = feat_mut_count[k] + 1
          
          # Set limit & direction of the MV search.
          limit = 0
          flip = 1
          if (cf[[k]] < x.interest[[k]]) {
            limit = min_feat_values[[k]]
            flip = -1
          }
          else {
            limit = max_feat_values[[k]]
            flip = 1
          }
          
          # Set increment.
          inc = (limit - cf[[k]]) / 10
          
          # Adjust increment for integer-valued features.
          if (data_feat_types[k] == "integer") {
            inc = round(inc)
            
            # If below 1, default to 1 & set steps to distance between feature
            # value & limit. 
            if (inc == 0) {
              inc = 1 * flip
            }
          }
          
          # Set maximum number of steps for MV search.
          max_steps = abs(limit - cf[[k]]) %/% abs(inc)
          
          writeLines(sprintf("Increment: %f", inc))
    
          # Mutate feature to search for closest MV.
          successful_steps = 0
          while (successful_steps < max_steps) {
            cf[k] = cf[k] + inc
            print(cf)
            
            result = pred$predict(cf)
            print(result)
            if (isNegativeClass(result, data_target_values, ml_alg_target_range)) {
              print("!MV! -- MONOTONICITY VIOLATION")
              break;
            }
  
            successful_steps = successful_steps + 1
          }
        }
        
        # Successful steps.
        writeLines(sprintf("Successful Steps: %f/%f", successful_steps, max_steps))
        print("\n###########################################################\n")
        
        # Update MV step data for resilience scores.
        feat_resilience_scores[k] = feat_resilience_scores[k] + (successful_steps / max_steps)
      }
    }
  
    # Add point of interest back to test data.
    data = insertRow(data, x.interest, i - 1)
  }
  
  # Finalise resilience score calculations.
  overall_res_score = 0
  total_mut_feat = 0
  total_mut_inst = 0
  for (key in names(feat_resilience_scores)) {
    if (feat_mut_count[[key]] > 0) {
      feat_resilience_scores[[key]] = feat_resilience_scores[[key]] / feat_mut_count[[key]]
      overall_res_score = overall_res_score + feat_resilience_scores[[key]]
      total_mut_feat = total_mut_feat + 1
      total_mut_inst = total_mut_inst + feat_mut_count[[key]]
    }
  }
  overall_res_score = overall_res_score / total_mut_feat
  
  # Results:
  # Overall resilience score.
  # Total count of counterfactuals.
  # Total count of invalid counterfactuals.
  # Total count of mutated features across all valid counterfactuals.
  # Hashmap of features to resilience scores.
  # Hashmap of features to count of mutated instances.
  
  return(monoTestResults("resilience" = overall_res_score,
                         "counterfactuals" = cf_count, 
                         "invalid" = cf_inv_count,
                         "mutated" = total_mut_inst,
                         "features" = feat_resilience_scores,
                         "instances" = feat_mut_count))
}

# Process a given set of data.
prepareData <- function(data, data_target) {
  # Omit rows with NA entries.
  data = na.omit(data)
  
  # Column names to lower case.
  names(data) = tolower(names(data))
  
  # Convert target column to factor.
  if (!is.factor(data[,data_target])) {
    data[,data_target] = factor(data[,data_target])
  }
  
  return(data)
}

# posTarg = gsub(" ", "", paste(adult_data@target, adult_data@target_values[POS_CLV_INDEX]))
# negTarg = gsub(" ", "", paste(adult_data@target, adult_data@target_values[NEG_CLV_INDEX]))
# 
# target_label = colnames(m)[colnames(m) == posTarg]
# target_value = 1
# if (identical(target_label, character(0))) {
#   target_label = colnames(m)[colnames(m) == negTarg]
#   target_value = 0
# }

###---- Set & preprocess data ----
for (ds in DATASETS) {
  data = prepareData(read.csv(ds@fp, stringsAsFactors = TRUE), ds@target)
  for (ml_alg in ML_ALGS) {
    # Test dataset w/ algorithm type.
    sink(sprintf("mono_tests_%s_%s.txt", ds@id, ml_alg@id))
    start_time = Sys.time()
    results = monoTest(data,
                       ds@id,
                       ds@target,
                       ds@target_values,
                       ds@nonactionable,
                       ml_alg@id,
                       ml_alg@target_range)
    end_time = Sys.time()
    writeLines(sprintf("Execution Time: %f", (end_time - start_time)))
    sink()
  }
}