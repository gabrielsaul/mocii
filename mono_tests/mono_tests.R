mono_tests <- function() {
  ############################################
  ### Monotonicity Tests
  ############################################
  
  # Datasets:
  # - German Credit Risk
  # - Diabetes
  # - Adult Income Census
  # - HELOC (FICO 2018)
  # - COMPAS Recidivism Racial Bias
  
  # AI Model Algorithms:
  # - Support Vector Machine (SVM)
  # - Neural Network (NN)
  # - Random Forest (RF)
  
  #--- Setup ----
  # Set the working directory.
  wdir = "C:\\Users\\Owner\\OneDrive\\Documents\\Academic\\UKC\\CS\\3\\All\\COMP6200 Research Project\\MONOMOC\\monomoc\\mono_tests"
  setwd(wdir)
  
  # To run MOC
  # load `iml` and `counterfactuals` like "normal" packages.
  devtools::load_all("../iml", export_all = FALSE)
  devtools::load_all("../counterfactuals", export_all = FALSE)
  
  # Load necessary library packages.
  library("mlr")
  library("mlrCPO")
  library("ggplot2")
  
  # Generated by irace in folder appendix_irace.
  best.params = readRDS("../saved_objects/best_configs.rds")  
  USE_TRAINED_MODEL = FALSE
  PARALLEL = TRUE
  
  # Datasets w/ class labels & non-actionable features.
  # German credit risk
  german_fp = "datasets/german_credit_data.csv"
  german_target = "risk"
  
  # Diabetes
  diabetes_fp = "datasets/diabetes.csv"
  
  # Adult Income Census
  adult_fp = "datasets/adult.csv"
  
  # HELOC (FICO 2018)
  fico_fp = "datasets/fico.csv"
  
  # COMPAS Recidivism Racial Bias
  compas_fp = "datasets/compas.csv"
  
  ###---- Get data ----
  credit = read.csv(german_fp, row.names = 1, stringsAsFactors = TRUE)
  names(credit)
  # omit rows with NA entries
  credit = na.omit(credit)
  # join groups with small frequencies
  levels(credit$Purpose) = c("others", "car", "others", "others",
                             "furniture", "radio/TV", "others", "others")
  levels(credit$Saving.accounts) = c("little", "moderate", "rich", "rich")
  # Colnames to lower
  names(credit) = tolower(names(credit))
  # Drop levels
  credit = droplevels.data.frame(credit)
  
  ###---- Test first 20 data points with 'bad' class label ----
  
  # Minimum credit.amount in set
  mincred <- min(credit["credit.amount"])
  
  # Counter for test data points
  counter <- 0
  
  # Number of monotonicity violations
  mv_count <- 0
  
  # Create output file
  file.create("nl_test_results.txt")
  fp <- file("nl_test_results.txt")
  
  for (i in 1:nrow(credit)) {
    # Exit loop
    if (counter >= 2) {
      break
    }
    else if (credit[i,]["risk"] == "bad") {
      counter <- counter + 1
      
      # Separate x.interest from training data
      x.interest = credit[i,]
      credit = credit[-i,]
      
      # Output to file
      print("Data Point:\n")
      print(x.interest)
      print("\n")
      
      ###---- Train model ----
      if (USE_TRAINED_MODEL) {
        credit.model = readRDS("model_svm.rds")
      } else {
        credit.task = makeClassifTask(id = "credit",
                                      data = credit, target = "risk")
        lrn = makeLearner("classif.svm", predict.type = "prob")
        credit.lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
        param.set = pSS(
          cost: numeric[0.01, 1]
        )
        
        TUNEITERS = 100L
        RESAMPLING = cv5
        
        if (PARALLEL) {
          parallelMap::parallelStartSocket(parallel::detectCores(), level = "mlr.tuneParams")
        }
        
        ctrl = makeTuneControlRandom(maxit = TUNEITERS * length(param.set$pars))
        lrn.tuning = makeTuneWrapper(lrn, RESAMPLING, list(mlr::acc), param.set, ctrl, show.info = FALSE)
        res = tuneParams(lrn, credit.task, RESAMPLING, par.set = param.set, control = ctrl,
                         show.info = FALSE)
        performance = resample(lrn.tuning, credit.task, RESAMPLING, list(mlr::acc))$aggr
        
        if (PARALLEL) {
          parallelMap::parallelStop()
        }
        credit.lrn = setHyperPars2(credit.lrn, res$x) 
        credit.model = mlr::train(credit.lrn, credit.task)  # use mlr:: in case caret is loaded somewhere
      }
      
      pred = Predictor$new(model = credit.model, data = credit, class = "good",
                           conditional = FALSE)
      ctr = partykit::ctree_control(maxdepth = 5L)
      
      set.seed(1234)
      pred$conditionals = fit_conditionals(pred$data$get.x(), ctrl = ctr)
      
      ###---- Compute counterfactuals ----
      res <- pred$predict(x.interest)
      print(res)
      
      set.seed(1000)
      system.time({credit.cf = Counterfactuals$new(predictor = pred, 
                                                   x.interest = x.interest, 
                                                   target = c(0.5, 1), epsilon = 0, generations = list(mosmafs::mosmafsTermStagnationHV(10),
                                                                                                       mosmafs::mosmafsTermGenerations(200)), 
                                                   mu = best.params$mu, 
                                                   p.mut = best.params$p.mut, p.rec = best.params$p.rec, 
                                                   p.mut.gen = best.params$p.mut.gen, 
                                                   p.mut.use.orig = best.params$p.mut.use.orig, 
                                                   p.rec.gen = best.params$p.rec.gen, initialization = "icecurve",
                                                   p.rec.use.orig = best.params$p.rec.use.orig)})
      
      # Number of counterfactuals
      nrow(credit.cf$results$counterfactuals)
      id = credit.cf$results$counterfactuals$dist.target == 0
      sum(id)
      
      # Focus counterfactuals that met target
      credit.cf$results$counterfactuals = credit.cf$results$counterfactuals[which(id), ]
      credit.cf$results$counterfactuals.diff = credit.cf$results$counterfactuals.diff[which(id), ]
      
      # Get relative frequency of feature changes
      credit.cf$get_frequency()
      
      # Output full list of counterfactuals
      print(credit.cf$results$counterfactuals)
  
      # Check counterfactuals for non-linearity
      for (j in 1:nrow(credit.cf$results$counterfactuals)) {
        
        # Remove additional columns
        cf <- credit.cf$results$counterfactuals[j,][-c(10:14)]
        
        # Calculate decrement value
        dec <- (cf["credit.amount"] - mincred) %/% 10
        
        # Test original counterfactual
        print(cf)
        res <- pred$predict(cf)
        print(res)
        
        # Monotonicity violation found
        if (res < 0.5) {
          mv_count <- mv_count + 1
          txt <- paste("MONOTONICITY VIOLATION #", toString(mv_count), sep="")
          print(txt)
        }
        
        # Reduce credit.amount and pass through classifier
        for (k in 1:10) {
          cf["credit.amount"] <- cf["credit.amount"] - dec
          
          print(cf)
          res <- pred$predict(cf)
          print(res)
          if (res < 0.5) {
            print("SERPENT SPOTTED")
          }
        }
        print("\n****************************************************\n")
      }
    
      
      # Add point of interest back to data frame
      credit[nrow(credit),] = x.interest
  
      
      ###---- Plots ----
      a = credit.cf$plot_parallel(features = c("duration", "credit.amount"), plot.x.interest = FALSE)
      a = a + scale_x_discrete(expand = c(0.1, 0.1), labels= c("duration", "credit amount"))
      a
      b = credit.cf$plot_surface(features = c("duration", "credit.amount"))
      b
      c = credit.cf$plot_hv()
      c
    }
  }
}

# debug(test_func)
mono_tests()