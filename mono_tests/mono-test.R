############################################
### Monotonicity Tests
############################################

#--- Setup ----
# Set the working directory.
wdir = "C:\\Users\\Owner\\OneDrive\\Documents\\Academic\\UKC\\CS\\3\\All\\COMP6200 Research Project\\MONOMOC\\monomoc\\mono_tests"
setwd(wdir)

# To run MOC
# load `iml` and `counterfactuals` like "normal" packages.
devtools::load_all("../iml", export_all = FALSE)
devtools::load_all("../counterfactuals", export_all = FALSE)

# Load helper functions.
source("mono_helpers/mono-helpers.R")

# Load necessary library packages.
library("mlr")
library("mlrCPO")
library("purrr")

# Generated by irace in folder appendix_irace.
best.params = readRDS("../saved_objects/best_configs.rds")  

# Data structs.

# Adult Income Census.
adult_data = dataStruct(df = read.csv("datasets/processed/adult_data_less.csv", 
                                      stringsAsFactors = TRUE),
                        id = "adult",
                        target = "class",
                        target_values = c("GT50K", "LTE50K"),
                        nonactionable = c("age", 
                                          "education", 
                                          "marital_status", 
                                          "relationship", 
                                          "race", 
                                          "sex", 
                                          "native_country"))

# COMPAS Recidivism Racial Bias.
compas_data = dataStruct(df = read.csv("datasets/processed/compas_data_less.csv", 
                                       stringsAsFactors = TRUE),
                         id = "compas",
                         target = "two_year_recid",
                         target_values = c("no", "yes"),
                         nonactionable = c("age", "age_cat", "sex", "race"))

# Diabetes.
diabetes_data = dataStruct(df = read.csv("datasets/processed/diabetes_data.csv", 
                                         stringsAsFactors = TRUE),
                           id = "diabetes",
                           target = "outcome",
                           target_values = c("neg", "pos"),
                           nonactionable = c("age", "pregnancies"))

# HELOC (FICO 2018).
fico_data = dataStruct(df = read.csv("datasets/processed/fico_data_less.csv", 
                                     stringsAsFactors = TRUE),
                       id = "fico",
                       target = "riskperformance",
                       target_values = c("Good", "Bad"),
                       nonactionable = c("externalriskestimate"))

# German credit risk.
german_data = dataStruct(df = read.csv("datasets/processed/german_credit_data.csv", 
                                       stringsAsFactors = TRUE),
                         id = "german",
                         target = "risk",
                         target_values = c("good", "bad"),
                         nonactionable = c("age", "sex"))

# ML Algorithm Types.
# Neural network (NN).
NN_ALG_ID = "nn"
nn_alg = mlAlgStruct(id = NN_ALG_ID,
                     target_range = c(0.5, 1.0))

# Random Forest (RF)
RF_ALG_ID = "rf"
rf_alg = mlAlgStruct(id = RF_ALG_ID,
                     target_range = c(0.5, 1.0))

# Support Vector Machine (SVM).
SVM_ALG_ID = "svm"
svm_alg = mlAlgStruct(id = SVM_ALG_ID,
                      target_range = c(0.5, 1.0))

# Full list of datasets.
DATASETS = list(german_data)

# Full list of ML algorithm types.
ML_ALGS = list(svm_alg)

# Main test function for monotonicity constraint violation proximity (resilience).
monoTest <- function(data,
                     id,
                     target,
                     target_values,
                     naf,
                     ml_alg_id,
                     ml_alg_target_range,
                     num_points_interest = 1,
                     data_feat_types = sapply(type.convert(data, as.is = TRUE), class)) 
  {
  
  # Max tuning iterations & resampling.
  TUNEITERS = 100L
  RESAMPLING = cv5
  
  # Count of counterfactuals (valid or not), for each (dataset, model) pair.
  cf_count = 0
  
  # Count of invalid CFs, for each (dataset, model) pair.
  cf_inv_count = 0  
  
  # Hashmap of feature names to resilience scores.
  feat_resilience_scores = getHashMap(names(data[,-which(names(data) == target)]))
  
  # Hashmap of feature names to count of mutated instances.
  feat_mut_count = getHashMap(names(feat_resilience_scores))
  
  # Hashmaps of feature names to successful & max steps.
  feat_succ_steps = getHashMap(names(feat_resilience_scores))
  feat_max_steps = getHashMap(names(feat_resilience_scores))
  
  # Counterfactual resilience scores.
  cf_resilience_scores = c()
  
  # Count of usable data samples tested.
  data_samples_tested = 0
  
  # Count of unusable data samples.
  data_samples_rejected = 0
  
  # Base sample set of all data points with negative class values.
  base_sample_set = which(data[,target] == target_values[NEG_CLV_INDEX])
  
  # Minimum & maximum values for each numeric feature in the test data.
  min_feat_values = apply(data, 2, min)
  min_feat_values = map_if(min_feat_values, isNumericString, as.numeric)
  max_feat_values = apply(data, 2, max)
  max_feat_values = map_if(max_feat_values, isNumericString, as.numeric)
  
  print("\nMIN FEAT VALUES:")
  print(min_feat_values)
  print("\nMAX FEAT VALUES:")
  print(max_feat_values)
  
  # Operate on points of interest.
  for (i in 1:num_points_interest) {
    
    # Initiate parellel tuning.
    parallelMap::parallelStartSocket(parallel::detectCores(), level = "mlr.tuneParams")
    
    # Task for classification.
    data.task = makeClassifTask(id = id,
                                data = data,
                                target = target,
                                positive = target_values[POS_CLV_INDEX])
    
    # Randomly sample a usable data point.
    is_usable = FALSE
    while (!is_usable) {
      
      # Select point of interest.
      set.seed(as.numeric(Sys.time()))
      sample_i = sample(1:length(base_sample_set), 1)
      x.interest = data[base_sample_set[sample_i],]
      
      # Remove point of interest from training data & base sample set.
      data = data[-base_sample_set[sample_i],]
      base_sample_set = base_sample_set[-sample_i]
      
      # Choose & train the model and set the predictor.
      pred = NULL
      if (ml_alg_id == NN_ALG_ID) {
        
        # Learner: Neural network.
        lrn = makeLearner("classif.nnet",
                          predict.type = "prob",
                          fix.factors.prediction = TRUE)
        
        # Normalisation/dummy encode.
        data.lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
        
        # Parameters for tuning.
        param_grid = makeParamSet(
          makeNumericParam("size", lower = 1, upper = 20),
          makeNumericParam("decay", lower = 0.1, upper = 0.9)
        )
        
        # Random search for tuning method.
        tune_control = makeTuneControlRandom(maxit = TUNEITERS)
        
        # Tune.
        data.lrn.tuned = tuneParams(data.lrn, 
                                    task = data.task, 
                                    resampling = RESAMPLING, 
                                    par.set = param_grid, 
                                    control = tune_control)
        
        # Train the model.
        data.model = mlr::train(data.lrn.tuned$learner, data.task)
        
        # Set as predictor.
        pred = Predictor$new(model = data.model,
                             data = data,
                             class = target_values[POS_CLV_INDEX])
      }
      else if (ml_alg_id == RF_ALG_ID) {
        
        # Learner: Random Forest.
        lrn = makeLearner("classif.randomForest", 
                           predict.type = "prob", 
                           fix.factors.prediction = TRUE)
        
        # Parameters for tuning.
        param_grid = makeParamSet(
          makeIntegerParam("ntree", lower = 50, upper = 500),
          makeIntegerParam("mtry", lower = 1, upper = ncol(data) - 1)
        )
        
        # Random search for tuning method.
        tune_control = makeTuneControlRandom(maxit = TUNEITERS)
        
        # Tune.
        lrn.tuned = tuneParams(lrn, 
                               task = data.task, 
                               resampling = RESAMPLING, 
                               par.set = param_grid, 
                               control = tune_control)
        
        # Train the model.
        data.model = mlr::train(lrn.tuned$learner, data.task)
        
        # Set as predictor.
        pred = Predictor$new(model = data.model,
                             data = data,
                             class = target_values[POS_CLV_INDEX])
      }
      else if (ml_alg_id == SVM_ALG_ID) {

        # Learner: Support Vector Machine.
        lrn = makeLearner("classif.svm", predict.type = "prob")
        
        # Normalisation/dummy encode.
        data.lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
        
        # Parameters for tuning.
        param.set = pSS(
          cost: numeric[0.01, 1]
        )
        
        # Tune.
        ctrl = makeTuneControlRandom(maxit = TUNEITERS * length(param.set$pars))
        lrn.tuning = makeTuneWrapper(lrn, RESAMPLING, list(mlr::acc), param.set, ctrl, show.info = FALSE)
        res = tuneParams(lrn, data.task, RESAMPLING, par.set = param.set, control = ctrl,
                         show.info = FALSE)
        performance = resample(lrn.tuning, data.task, RESAMPLING, list(mlr::acc))$aggr
        data.lrn = setHyperPars2(data.lrn, res$x) 
        
        # Train the model.
        data.model = mlr::train(data.lrn, data.task)
        
        # Set as predictor.
        pred = Predictor$new(model = data.model, 
                             data = data, 
                             class = target_values[POS_CLV_INDEX],
                             conditional = FALSE)
        
        # Fit conditional inference trees.
        ctr = partykit::ctree_control(maxdepth = 5L)
        set.seed(1234)
        pred$conditionals = fit_conditionals(pred$data$get.x(), ctrl = ctr)
      }
      else {
        stop("Error: Invalid ML algorithm ID")
      }
      
      # Verify that the point of interest is predicted as negative class.
      print(x.interest)
      result = pred$predict(x.interest)
      print(result)
      
      if (isNegativeClass(result, ml_alg_target_range)) {
        is_usable = TRUE
      }
      else {
        # Data point unusable.
        data_samples_rejected = data_samples_rejected + 1
        
        # No more usable samples. Terminate.
        if (length(base_sample_set) == 0) {
          warning("NOTE: Terminated before full sample set was tested.")
          return(generateMonoTestResults(feat_resilience_scores,
                                         cf_resilience_scores,
                                         cf_count,
                                         cf_inv_count,
                                         feat_mut_count,
                                         data_samples_tested,
                                         data_samples_rejected,
                                         feat_succ_steps,
                                         feat_max_steps))
        }
        
        # Add data point back to test data.
        data = insertRow(data, x.interest, sample_i - 1)
      }
    }
    parallelMap::parallelStop()
    
    # Point of interest selected.
    data_samples_tested = data_samples_tested + 1
    
    print("\nPoint of Interest: ")
    print(x.interest)
    
    # Compute counterfactuals for data point of interest.
    set.seed(1000)
    system.time({data.cf = Counterfactuals$new(predictor = pred, 
                                               x.interest = x.interest,
                                               target = ml_alg_target_range, epsilon = 0, generations = list(mosmafs::mosmafsTermStagnationHV(10),
                                                                                                             mosmafs::mosmafsTermGenerations(200)), 
                                               mu = best.params$mu, 
                                               p.mut = best.params$p.mut, p.rec = best.params$p.rec, 
                                               p.mut.gen = best.params$p.mut.gen, 
                                               p.mut.use.orig = best.params$p.mut.use.orig, 
                                               p.rec.gen = best.params$p.rec.gen, initialization = "icecurve",
                                               p.rec.use.orig = best.params$p.rec.use.orig,
                                               fixed.features = naf)})
    
    # Increment number of counterfactuals.
    num_cf = nrow(data.cf$results$counterfactuals)
    cf_count = cf_count + num_cf
    id = data.cf$results$counterfactuals$dist.target == 0
    sum(id)
    
    print("Counterfactuals: ")
    print(data.cf$results$counterfactuals)
    
    # Count invalid counterfactuals & remove from set.
    data.cf$results$counterfactuals = data.cf$results$counterfactuals[which(id), ]
    data.cf$results$counterfactuals.diff = data.cf$results$counterfactuals.diff[which(id), ]
    cf_inv_count = cf_inv_count + (num_cf - nrow(data.cf$results$counterfactuals))
    
    # Get relative frequency of feature changes.
    data.cf$get_frequency()
    
    # For each counterfactual...
    for (j in 1:nrow(data.cf$results$counterfactuals)) {
      
      # Remove additional columns.
      cf = data.cf$results$counterfactuals[j,][,1:(ncol(data.cf$results$counterfactuals[j,]) - 5)]
      
      print("\nOriginal CF:")
      print(cf)
      
      # Test original counterfactual.
      result = pred$predict(cf)
      print(result)
      
      # Calculate mutated numeric features.
      tmp_x = x.interest[,-which(names(x.interest) == target)]
      changed = getMutNumericFeatures(cf, tmp_x)
      writeLines(sprintf("\nNumber of Mutated Features: %f", length(changed)))
      print("\nMutated Features:")
      print(changed)
      
      # For each mutated numeric feature...
      cf_resilience_score = 0.0
      for (k in changed) {
        # Successful step ratio.
        step_ratio = 0
        
        # Increment count of instances of mutated feature.
        feat_mut_count[k] = feat_mut_count[k] + 1
        
        # If already at min/max, skip.
        if (cf[[k]] > min_feat_values[[k]] & cf[[k]] < max_feat_values[[k]]) {
          writeLines(sprintf("\nMutated Feature: %s", k))
          
          # Set limit & direction of the MV search.
          limit = 0
          flip = 1
          if (cf[[k]] < x.interest[[k]]) {
            limit = min_feat_values[[k]]
            flip = -1
          }
          else {
            limit = max_feat_values[[k]]
            flip = 1
          }
          
          # Set increment.
          inc = (limit - cf[[k]]) / 10
          
          # Adjust increment for integer-valued features.
          if (data_feat_types[k] == "integer") {
            inc = round(inc)
            
            # If below 1, default to 1.
            if (inc == 0) {
              inc = 1 * flip
            }
          }
          
          # Set maximum number of steps for MV search.
          max_steps = abs(limit - cf[[k]]) %/% abs(inc)
          print("\nMAX_STEPS:")
          print(max_steps)
          
          writeLines(sprintf("\nIncrement: %f", inc))
          
          # Mutate feature to search for closest MV.
          successful_steps = 0
          while (successful_steps < max_steps) {
            cf[k] = cf[k] + inc
            print(cf)
            
            result = pred$predict(cf)
            print(result)
            if (isNegativeClass(result, ml_alg_target_range)) {
              print("!MV! -- MONOTONICITY VIOLATION")
              break;
            }
            
            successful_steps = successful_steps + 1
          }
          
          # Log step ratio.
          step_ratio = successful_steps / max_steps
          feat_succ_steps[[k]] = feat_succ_steps[[k]] + successful_steps
          feat_max_steps[[k]] = feat_max_steps[[k]] + max_steps
          
          writeLines(sprintf("Successful Steps: %d/%d", successful_steps, max_steps))
          print("\n###########################################################\n")
        }
        else {
          # Feature is already at min/max: Assume full resilience.
          print("Already at min/max.")
          print("Full resilience.")
          step_ratio = 1.0
        }
        
        # Update MV step data for resilience scores.
        feat_resilience_scores[k] = feat_resilience_scores[k] + step_ratio
        cf_resilience_score = cf_resilience_score + step_ratio
      }
      
      # Final counterfactual resilience score.
      if (length(changed) > 0) {
        cf_resilience_scores[length(cf_resilience_scores) + 1] = cf_resilience_score / length(changed)
      }
      else {
        cf_resilience_scores[length(cf_resilience_scores) + 1] = NaN
      }
    }
    
    # Add point of interest back to test data.
    data = insertRow(data, x.interest, i - 1)
  }
  
  return(generateMonoTestResults(feat_resilience_scores,
                                 cf_resilience_scores,
                                 cf_count,
                                 cf_inv_count,
                                 feat_mut_count,
                                 data_samples_tested,
                                 data_samples_rejected,
                                 feat_succ_steps,
                                 feat_max_steps))
}

# Test datasets x ML alg types.
for (ds in DATASETS) {
  # Prepare data.
  names(ds@df) = tolower(names(ds@df))
  ds@df = na.omit(ds@df)
  ds@df[,ds@target] = as.factor(ds@df[,ds@target])
  
  for (ml_alg in ML_ALGS) {
    sink(sprintf("logs/mono_tests_%s_%s.txt", ds@id, ml_alg@id))
    start_time = Sys.time()
    results = monoTest(ds@df,
                       ds@id,
                       ds@target,
                       ds@target_values,
                       ds@nonactionable,
                       ml_alg@id,
                       ml_alg@target_range)
    end_time = Sys.time()
    print("\nRESULTS: ")
    print(results)
    writeLines(sprintf("Execution Time: %f", (end_time - start_time)))
    sink()
  }
}